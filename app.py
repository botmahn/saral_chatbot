import os
import argparse
import json
import re
import tempfile
from datetime import datetime
from typing import List, Dict, Any, Tuple, Optional

import streamlit as st
import chromadb

from langchain_community.vectorstores import Chroma
from langchain_community.embeddings import HuggingFaceBgeEmbeddings
from langchain_ollama import ChatOllama
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser
from langchain_core.runnables import RunnableParallel
from langsmith.run_helpers import traceable

# --- Optional external evaluation libraries (ROUGE & BERTScore) ---
try:
    from rouge_score import rouge_scorer
    HAS_ROUGE = True
except ImportError:
    HAS_ROUGE = False

try:
    from bert_score import score as bert_score_fn
    HAS_BERTSCORE = True
except ImportError:
    HAS_BERTSCORE = False

# --- Optional: PyMuPDF for PDF ingestion ---
try:
    import fitz  # PyMuPDF
    HAS_PYMUPDF = True
except ImportError:
    HAS_PYMUPDF = False

# --- Import your project classes ---
from saral.processors.extract_from_latex import LatexProjectParser  # contains LatexProjectParser
from saral.chunkers.chunk_text import JSONTextChunker              # contains JSONTextChunker
from saral.embedders.embed_text import ChromaJSONIndexer           # contains ChromaJSONIndexer

# LangChain / LangSmith config
os.environ["LANGCHAIN_TRACING_V2"] = "true"
os.environ["LANGCHAIN_PROJECT"] = "saral"


# ===================================================================
# Helpers: PDF ‚Üí JSON (compatible with JSONTextChunker expectations)
# ===================================================================
def parse_pdf_to_json(pdf_path: str, out_json_path: str) -> None:
    """
    Parse a PDF into a simple JSON structure:
    {
        "paper_title": "...",
        "sections": [
            {
                "section_title": "Page 1",
                "text": "<full page text>",
                "page": 1
            },
            ...
        ]
    }

    This is intentionally simple so that JSONTextChunker can treat each page
    as a 'section'. For the assignment, full robustness is not required.
    """
    if not HAS_PYMUPDF:
        raise ImportError(
            "PyMuPDF (fitz) is not installed. Install with `pip install pymupdf` to ingest PDFs."
        )

    doc = fitz.open(pdf_path)
    sections = []
    for i, page in enumerate(doc, start=1):
        text = page.get_text("text")
        if text and text.strip():
            sections.append(
                {
                    "section_title": f"Page {i}",
                    "text": text,
                    "page": i,
                }
            )

    data = {
        "paper_title": os.path.basename(pdf_path),
        "sections": sections,
    }

    with open(out_json_path, "w", encoding="utf-8") as f:
        json.dump(data, f, ensure_ascii=False, indent=2)


# ===================================================================
# Talk Evaluator: Provenance + Factuality Proxy + ROUGE/BERTScore
# ===================================================================
class TalkEvaluator:
    """
    Evaluation utilities:
    1. Citation Coverage: % of lines with a [CHUNK_X] citation.
    2. Factuality proxy: lexical overlap between generated lines and their cited chunks.
    3. Optional: ROUGE & BERTScore vs reference script (if libs installed).
    """

    CITATION_PATTERN = re.compile(r"\[CHUNK_(\d+)\b.*?\]")

    @staticmethod
    def _lexical_overlap(a: str, b: str) -> float:
        """
        Simple lexical overlap score between two strings:
        |tokens(a) ‚à© tokens(b)| / |tokens(a)|
        (clipped to [0, 1]). This is a lightweight factuality proxy.
        """
        tokens_a = set(re.findall(r"\w+", a.lower()))
        tokens_b = set(re.findall(r"\w+", b.lower()))
        if not tokens_a or not tokens_b:
            return 0.0
        inter = tokens_a & tokens_b
        return len(inter) / max(len(tokens_a), 1)

    @staticmethod
    def evaluate_provenance(
        generated_text: str,
        retrieved_docs: List[Any],
    ) -> Dict[str, Any]:
        """
        Evaluate provenance & a factuality proxy using retrieved_docs.

        Args:
            generated_text: Full markdown generated by the model.
            retrieved_docs: List of langchain Document objects used as context.

        Returns:
            Dictionary with:
            - citation_coverage_score
            - total_lines_analyzed
            - unique_chunks_cited
            - avg_lexical_overlap_cited_lines
        """
        lines = [
            line.strip()
            for line in generated_text.split("\n")
            if len(line.strip()) > 20  # ignore very short lines
        ]

        if not lines:
            return {
                "citation_coverage_score": 0.0,
                "total_lines_analyzed": 0,
                "unique_chunks_cited": [],
                "avg_lexical_overlap_cited_lines": 0.0,
                "details": "No meaningful text generated.",
            }

        # Map CHUNK index -> corresponding retrieved doc
        chunk_index_to_doc: Dict[int, Any] = {}
        for idx, doc in enumerate(retrieved_docs, start=1):
            chunk_index_to_doc[idx] = doc

        cited_lines = 0
        citations_found: List[int] = []
        lexical_scores: List[float] = []

        for line in lines:
            matches = TalkEvaluator.CITATION_PATTERN.findall(line)
            if matches:
                cited_lines += 1
                # Remove citation tags when computing lexical overlap
                clean_line = TalkEvaluator.CITATION_PATTERN.sub("", line)
                for m in matches:
                    try:
                        chunk_id = int(m)
                    except ValueError:
                        continue
                    citations_found.append(chunk_id)
                    doc = chunk_index_to_doc.get(chunk_id)
                    if doc is not None:
                        score = TalkEvaluator._lexical_overlap(
                            clean_line, doc.page_content
                        )
                        lexical_scores.append(score)

        coverage = cited_lines / len(lines) if lines else 0.0
        avg_lexical = sum(lexical_scores) / len(lexical_scores) if lexical_scores else 0.0

        return {
            "citation_coverage_score": round(coverage, 3),
            "total_lines_analyzed": len(lines),
            "unique_chunks_cited": sorted(set(citations_found)),
            "avg_lexical_overlap_cited_lines": round(avg_lexical, 3),
        }

    @staticmethod
    def compute_rouge_and_bertscore(
        generated: str,
        reference: str,
    ) -> Dict[str, Any]:
        """
        Compute ROUGE and BERTScore between generated and reference scripts.
        Requires `rouge_score` and `bert_score` libraries.

        Returns:
            Dictionary with ROUGE-L and BERTScore F1 (if available).
        """
        results: Dict[str, Any] = {}

        # ROUGE-L
        if HAS_ROUGE:
            scorer = rouge_scorer.RougeScorer(["rougeL"], use_stemmer=True)
            scores = scorer.score(reference, generated)
            rouge_l = scores["rougeL"].fmeasure
            results["rougeL_f"] = round(rouge_l, 3)
        else:
            results["rougeL_f"] = None
            results["rougeL_note"] = "rouge_score library not installed."

        # BERTScore (F1)
        if HAS_BERTSCORE:
            P, R, F1 = bert_score_fn(
                [generated],
                [reference],
                lang="en",
                rescale_with_baseline=True,
            )
            results["bertscore_f1"] = round(float(F1.mean().item()), 3)
        else:
            results["bertscore_f1"] = None
            results["bertscore_note"] = "bert_score library not installed."

        return results


# -------------------------------------------------------------------
# Orchestrator: LaTeX/PDF ingestion -> JSON -> chunks -> Chroma
# -------------------------------------------------------------------
def ingest_latex_and_pdf_and_build_db(
    uploaded_files: List[st.runtime.uploaded_file_manager.UploadedFile],
    persist_dir: str,
    collection_name: str,
    top_k: int,
    chunk_size: int = 256,
    chunk_overlap: int = 64,
    batch_size: int = 32,
):
    """
    Full ingestion pipeline:
    1. Save uploaded files (LaTeX or PDF) to disk.
    2. For .tex:
       - LatexProjectParser -> base.json, base_images.json, etc.
       - Use base.json for text.
       For .pdf:
       - parse_pdf_to_json(...) to create a compatible JSON.
    3. Chunk text JSON -> chunked JSON (JSONTextChunker).
    4. Embed chunks -> ChromaDB (ChromaJSONIndexer).
    5. Build LangChain retriever + return vectorstore.
    """

    parsed_json_dir = os.path.join(persist_dir, "parsed_json")
    chunked_json_dir = os.path.join(persist_dir, "chunked_json")
    os.makedirs(parsed_json_dir, exist_ok=True)
    os.makedirs(chunked_json_dir, exist_ok=True)

    parser = LatexProjectParser(verbose=True)
    text_json_paths: List[str] = []

    for uf in uploaded_files:
        filename = uf.name
        ext = os.path.splitext(filename)[1].lower()

        if ext == ".tex":
            # ---- LaTeX ingestion ----
            suffix = ".tex"
            with tempfile.NamedTemporaryFile(delete=False, suffix=suffix) as tmp_tex:
                tmp_tex.write(uf.getvalue())
                tmp_tex_path = tmp_tex.name

            base_name = os.path.splitext(filename)[0]
            base_output_path = os.path.join(parsed_json_dir, base_name)

            # Writes: base.json, base_images.json, base_equations.json, base_tables.json
            parser.parse_and_save(tmp_tex_path, base_output_path)
            text_json_path = base_output_path + ".json"
            text_json_paths.append(text_json_path)

            os.remove(tmp_tex_path)

        elif ext == ".pdf":
            # ---- PDF ingestion ----
            if not HAS_PYMUPDF:
                raise ImportError(
                    "PyMuPDF (fitz) not installed. Install with `pip install pymupdf` "
                    "to enable PDF ingestion."
                )

            with tempfile.NamedTemporaryFile(delete=False, suffix=".pdf") as tmp_pdf:
                tmp_pdf.write(uf.getvalue())
                tmp_pdf_path = tmp_pdf.name

            base_name = os.path.splitext(filename)[0]
            base_output_path = os.path.join(parsed_json_dir, base_name + "_pdf")
            text_json_path = base_output_path + ".json"

            parse_pdf_to_json(tmp_pdf_path, text_json_path)
            text_json_paths.append(text_json_path)

            os.remove(tmp_pdf_path)

        else:
            # Skip unknown file types
            continue

    # 2. Chunk text JSON
    chunker = JSONTextChunker(
        chunk_size=chunk_size,
        chunk_overlap=chunk_overlap,
        encoding="cl100k_base",
        verbose=True,
    )

    chunked_json_paths: List[str] = []
    for in_path in text_json_paths:
        filename = os.path.basename(in_path)
        out_path = os.path.join(chunked_json_dir, filename)
        chunker.process(in_path, out_path)
        chunked_json_paths.append(out_path)

    # 3. Embed & index into Chroma
    indexer = ChromaJSONIndexer(
        persist_directory=persist_dir,
        collection_name=collection_name,
        model_name="sentence-transformers/all-MiniLM-L6-v2",
        batch_size=batch_size,
        verbose=True,
    )
    indexer.index_many(chunked_json_paths)

    # 4. Build LangChain retriever/vectorstore
    retriever, vectorstore = build_langchain_retriever(persist_dir, collection_name, top_k)
    return retriever, vectorstore


def build_langchain_retriever(persist_dir, collection_name, top_k):
    embeddings = HuggingFaceBgeEmbeddings(
        model_name="sentence-transformers/all-MiniLM-L6-v2",
        encode_kwargs={"normalize_embeddings": True},
    )

    client = chromadb.PersistentClient(path=persist_dir)

    try:
        vectorstore = Chroma(
            client=client,
            collection_name=collection_name,
            embedding_function=embeddings,
        )
        if not vectorstore.get()["ids"]:
            return None, None
    except Exception:
        return None, None

    # üîë Use MMR instead of plain similarity
    retriever = vectorstore.as_retriever(
        search_type="mmr", #Maximal Marginal Relevance
        search_kwargs={
            "k": top_k,          # how many chunks you finally want
            "fetch_k": top_k * 4,  # how many to consider before pruning (tune this)
            "lambda_mult": 0.5,  # 0.0 = max diversity, 1.0 = max relevance
        },
    )
    return retriever, vectorstore

# ===================================================================
# Context Builder with Provenance (section + page)
# ===================================================================
def docs_to_context_with_provenance(docs: List[Any]) -> str:
    """
    Turn retrieved docs into a context string with rich provenance.

    Each chunk is encoded as:

    [CHUNK_1 | Section: <section_title> | Source: <paper_title>, Page: <page>, Orig_Idx: <chunk_index>]
    <chunk text...>
    """
    parts = []
    for idx, d in enumerate(docs, start=1):
        meta = d.metadata or {}

        chunk_idx = meta.get("chunk_index", "N/A")
        paper_title = meta.get("paper_title", "Unknown Paper")
        page_num = meta.get("page", "N/A")
        section_title = meta.get("section_title", meta.get("section", "Unknown Section"))

        provenance_id = f"CHUNK_{idx}"
        header = (
            f"[{provenance_id} | Section: {section_title} | "
            f"Source: {paper_title}, Page: {page_num}, Orig_Idx: {chunk_idx}]"
        )

        content = d.page_content.replace("\n", " ")
        parts.append(f"{header}\n{content}")

    return "\n\n".join(parts)


# ===================================================================
# RAG Chain Builder
# ===================================================================
def build_talk_rag_chain(retriever, ollama_model):
    """
    Build a RAG chain with:
    - Retriever
    - System prompt
    - ChatOllama LLM
    """
    llm = ChatOllama(
        model=ollama_model,
        temperature=0.1,
        keep_alive="5m",
    )

    system_prompt = """You are SARAL, a helpful academic talk assistant.
You are given:
1. A user request about creating or refining a talk based on research papers.
2. A set of retrieved context chunks, labeled [CHUNK_1], [CHUNK_2], etc.

Your Instructions:
- Factuality: Use ONLY the provided context. If the information is not present, say so explicitly.
- Provenance: You MUST cite your sources. Every major claim, bullet point, or script sentence must end with the relevant tag, e.g.,
  "The transformer architecture allows parallelization [CHUNK_1 | Section: Introduction]."
- Math: Preserve all equations in LaTeX format, e.g., $E=mc^2$ or $$\\mathcal{{L}}_{{task}}$$.
- Style & Length: Adhere strictly to the requested style (Technical, Plain English, Press Release) and target duration (30s, 90s, 5mins).
- Accessibility: When style is Plain-English or Press Release, avoid jargon, explain acronyms on first use, and keep language accessible.
- Safety: No offensive or harmful content.

Context:
{context}
"""


    generation_prompt = ChatPromptTemplate.from_messages(
        [
            ("system", system_prompt),
            (
                "human",
                """User request: {user_request}
Target Length: {length_choice}
Target Style: {style}

Produce the output in this Markdown structure:

## Slides Overview
- Slide 1: Title [CHUNK_X]
- Slide 2: Title [CHUNK_Y]
...

## Slide Details
### Slide 1: [Title]
- Bullet point [CHUNK_X]
- Bullet point [CHUNK_Y]

### Slide 2: [Title]
...

## Speaker Script (Approx {length_choice})
Write a cohesive speech. Ensure you cite chunks in the flow of speech like
"...as shown in [CHUNK_1 | Section: Methods]."

## Speaker Notes & Math
### Slide 1 Notes
- Note with math: $x^2$ [CHUNK_Z]

### Slide 2 Notes
...

Remember: Every substantial claim must carry a provenance tag.
""",
            ),
        ]
    )

    rag_chain = (
        RunnableParallel(
            {
                "context": (lambda x: x["user_request"])
                | retriever
                | docs_to_context_with_provenance,
                "user_request": lambda x: x["user_request"],
                "length_choice": lambda x: x["length_choice"],
                "style": lambda x: x["style"],
            }
        )
        | generation_prompt
        | llm
        | StrOutputParser()
    )

    # Return components so we can manually stream in Streamlit
    return rag_chain, llm, generation_prompt


# ===================================================================
# Refinement (Change-Tracking)
# ===================================================================
@traceable(name="saral_refine")
def refine_talk(
    previous_output: str,
    refine_instruction: str,
    style: str,
    llm_model: str,
    stream_callback=None,
) -> str:
    """
    Refinement logic with left-to-right streaming.

    - Uses the same change-tracking format:
      **[OLD] <old text>** -> **[NEW] <new text>**
      plus a 'Change Rationale' section.
    - If stream_callback is provided, it will be called with the
      progressively growing response string, so the caller (Streamlit)
      can update the UI incrementally.
    - Returns the full final response as a string.
    """
    llm = ChatOllama(model=llm_model, temperature=0.2)

    prompt = ChatPromptTemplate.from_template(
        """
You are an expert editor.
User wants to refine the following presentation content.

Constraints:
1. Modify ONLY the parts requested by the user.
2. For key changes, explicitly show the delta like:
   **[OLD] <old text>** -> **[NEW] <new text>**.
3. At the end, add a 'Change Rationale' section explaining what and why you changed.
4. Preserve all existing provenance tags like [CHUNK_1 | Section: ...].

=== CURRENT CONTENT ===
{previous_output}
=======================

User Instruction: {refine_instruction}
Style constraint: {style}

Output the full updated markdown.
"""
    )

    # Build messages for the chat model
    prompt_value = prompt.format_prompt(
        previous_output=previous_output,
        refine_instruction=refine_instruction,
        style=style,
    )
    messages = prompt_value.to_messages()

    # Stream tokens left-to-right
    full_response = ""
    for chunk in llm.stream(messages):
        delta = getattr(chunk, "content", "") or ""
        full_response += delta
        if stream_callback is not None:
            # Let the caller (Streamlit) update the UI incrementally
            stream_callback(full_response)

    return full_response


# ===================================================================
# Logging helpers & Session State
# ===================================================================
def append_to_conversation_log(log_path, role, content, extra=None):
    """
    Append a conversation entry to a JSONL log file.
    """
    record: Dict[str, Any] = {
        "timestamp": datetime.utcnow().isoformat(),
        "role": role,
        "content": content,
    }
    if extra:
        record.update(extra)

    os.makedirs(
        os.path.dirname(log_path) if os.path.dirname(log_path) else ".", exist_ok=True
    )

    with open(log_path, "a", encoding="utf-8") as f:
        f.write(json.dumps(record, ensure_ascii=False) + "\n")


def init_session_state():
    """
    Initialize Streamlit session state variables.
    """
    keys = [
        "messages",
        "last_output",
        "rag_chain",
        "retriever",
        "vectorstore",
        "llm",
        "generation_prompt",
        "last_retrieved_docs",
    ]
    for k in keys:
        if k not in st.session_state:
            st.session_state[k] = None
    if st.session_state.messages is None:
        st.session_state.messages = []


# ===================================================================
# Retrieval Debug Helper
# ===================================================================
def log_retrieval_examples(query: str, vectorstore: Chroma, top_k: int = 4):
    """
    Log example retrieved chunks and similarity scores to stdout and
    return them for UI display (for debugging/evaluation).
    """
    try:
        results = vectorstore.similarity_search_with_score(query, k=top_k)
    except Exception as e:
        print(f"[retrieval] Error computing similarity scores: {e}")
        return None

    print(f"\n[retrieval] Top-{top_k} results for query: {query!r}")
    logged = []
    for i, (doc, score) in enumerate(results, start=1):
        meta = doc.metadata or {}
        section_title = meta.get("section_title", meta.get("section", "Unknown"))
        chunk_idx = meta.get("chunk_index", "N/A")
        snippet = doc.page_content.replace("\n", " ")[:200]
        print(
            f"[retrieval] #{i} score={score:.4f} "
            f"section={section_title!r} chunk_index={chunk_idx} "
            f"text={snippet!r}..."
        )
        logged.append((doc, score))

    return logged


# ===================================================================
# Streamlit App
# ===================================================================
def main(args):
    st.set_page_config(page_title="SARAL Chatbot", layout="wide", page_icon="üéì")
    st.title("üéì SARAL: Academic Talk Generator")

    init_session_state()

    # ----------------------------- Sidebar -----------------------------
    with st.sidebar:
        st.subheader("üìÑ Ingestion (LaTeX / PDF)")

        uploaded_files = st.file_uploader(
            "Upload main paper file(s) (.tex or .pdf)",
            type=["tex", "pdf"],
            accept_multiple_files=True,
        )
        ingest_clicked = st.button("Ingest Paper(s) into RAG DB")

        st.markdown("---")
        st.header("‚öôÔ∏è RAG Configuration")

        persist_dir = st.text_input(
            "Chroma Path",
            value=st.session_state.get("persist_dir", args.chroma_dir),
            key="persist_dir",
        )
        collection_name = st.text_input(
            "Collection Name",
            value=st.session_state.get("collection_name", args.collection_name),
            key="collection_name",
        )
        ollama_model = st.text_input(
            "Ollama Model",
            value=st.session_state.get("ollama_model", args.ollama_model),
            key="ollama_model",
        )
        top_k = st.slider(
            "Retrieval Chunk Count (k)",
            2,
            10,
            int(st.session_state.get("top_k", 4)),
            key="top_k",
        )

        st.markdown("---")

        # Ingestion (appears above configs, but uses current config values)
        if uploaded_files and ingest_clicked:
            with st.spinner("Parsing, chunking, embedding into Chroma..."):
                try:
                    retriever, vectorstore = ingest_latex_and_pdf_and_build_db(
                        uploaded_files=uploaded_files,
                        persist_dir=persist_dir,
                        collection_name=collection_name,
                        top_k=top_k,
                        chunk_size=256,
                        chunk_overlap=64,
                        batch_size=32,
                    )
                    if retriever is None:
                        st.error(
                            "Ingestion finished but retriever is empty. "
                            "Check your inputs and JSON outputs."
                        )
                    else:
                        rag_chain, llm, generation_prompt = build_talk_rag_chain(
                            retriever, ollama_model
                        )
                        st.session_state.vectorstore = vectorstore
                        st.session_state.retriever = retriever
                        st.session_state.rag_chain = rag_chain
                        st.session_state.llm = llm
                        st.session_state.generation_prompt = generation_prompt
                        st.session_state.last_output = None  # reset refine state
                        st.session_state.last_retrieved_docs = None
                        st.success(
                            f"Ingested {len(uploaded_files)} file(s) into collection '{collection_name}'."
                        )
                except Exception as e:
                    st.error(f"Error during ingestion: {e}")

        if st.button("Re-initialize Retriever from DB"):
            retriever, vectorstore = build_langchain_retriever(
                persist_dir, collection_name, top_k
            )
            if retriever:
                rag_chain, llm, generation_prompt = build_talk_rag_chain(
                    retriever, ollama_model
                )
                st.session_state.retriever = retriever
                st.session_state.vectorstore = vectorstore
                st.session_state.rag_chain = rag_chain
                st.session_state.llm = llm
                st.session_state.generation_prompt = generation_prompt
                st.success("Retriever loaded from existing ChromaDB.")
            else:
                st.error("Collection empty or not found. Please ingest papers first.")

    # ----------------------------- Tabs -----------------------------
    tab_gen, tab_eval = st.tabs(["üí¨ Generator", "üìä Evaluation"])

    # =========================== Generator Tab ===========================
    with tab_gen:
        col1, col2 = st.columns([3, 1])
        with col1:
            length_choice = st.radio(
                "Duration", ["30s", "90s", "5mins"], horizontal=True
            )
        with col2:
            style = st.selectbox(
                "Style", ["Technical", "Plain-English", "Press Release"]
            )

        # Container for chat history and current turn
        chat_container = st.container()

        # 1Ô∏è‚É£ Render existing chat messages above input
        with chat_container:
            for msg in st.session_state.messages:
                with st.chat_message(msg["role"]):
                    st.markdown(msg["content"])

        # 2Ô∏è‚É£ Input box at bottom
        user_input = st.chat_input("e.g., 'Make a 5-slide talk on the methodology...'")

        # 3Ô∏è‚É£ Handle new user input and stream assistant response
        if user_input:
            if not st.session_state.rag_chain or not st.session_state.retriever:
                st.error("Please ingest paper(s) or initialize the retriever first.")
            else:
                st.session_state.messages.append({"role": "user", "content": user_input})
                append_to_conversation_log(
                    args.saral_conversation_log,
                    "user",
                    user_input,
                    {"length": length_choice, "style": style},
                )

                with chat_container:
                    # Re-render the user turn explicitly
                    with st.chat_message("user"):
                        st.write(user_input)

                    with st.chat_message("assistant"):
                        placeholder = st.empty()

                        with st.spinner("SARAL is generating your talk..."):
                            try:
                                # --- First-time generation (streaming) ---
                                if st.session_state.last_output is None:
                                    retriever = st.session_state.retriever
                                    llm = st.session_state.llm
                                    generation_prompt = st.session_state.generation_prompt
                                    vectorstore = st.session_state.vectorstore

                                    # 1) Debug: log retrieval examples + similarity scores
                                    logged_results = None
                                    if vectorstore is not None:
                                        logged_results = log_retrieval_examples(
                                            user_input, vectorstore, top_k=top_k
                                        )

                                    if logged_results:
                                        with st.expander("üîç Retrieval debug (top-k chunks)"):
                                            for i, (doc, score) in enumerate(
                                                logged_results, start=1
                                            ):
                                                meta = doc.metadata or {}
                                                section_title = meta.get(
                                                    "section_title",
                                                    meta.get("section", "Unknown"),
                                                )
                                                chunk_idx = meta.get("chunk_index", "N/A")
                                                st.markdown(
                                                    f"**#{i}** | Score: `{score:.4f}` | "
                                                    f"Section: `{section_title}` | Chunk: `{chunk_idx}`"
                                                )
                                                st.markdown(doc.page_content[:500] + "...\n")

                                    # 2) Retrieve docs for actual context
                                    docs = retriever.invoke(user_input)
                                    st.session_state.last_retrieved_docs = docs
                                    context_str = docs_to_context_with_provenance(docs)

                                    # 3) Build messages for ChatOllama
                                    prompt_value = generation_prompt.format_prompt(
                                        user_request=user_input,
                                        length_choice=length_choice,
                                        style=style,
                                        context=context_str,
                                    )
                                    messages = prompt_value.to_messages()

                                    # 4) Stream tokens left-to-right
                                    full_response = ""
                                    for chunk in llm.stream(messages):
                                        delta = getattr(chunk, "content", "") or ""
                                        full_response += delta
                                        # Update Streamlit UI incrementally
                                        placeholder.markdown(full_response)
                                        # Print to console as it arrives
                                        print(delta, end="", flush=True)

                                    print()  # newline after stream
                                    response = full_response
                                    mode = "generate"

                                # --- Refinement mode (non-streaming) ---
                                else:
                                    # Refinement mode: stream left-to-right just like initial generation
                                    response = refine_talk(
                                        previous_output=st.session_state.last_output,
                                        refine_instruction=user_input,
                                        style=style,
                                        llm_model=ollama_model,
                                        stream_callback=lambda text: placeholder.markdown(text),
                                    )
                                    mode = "refine"

                                # Update session state
                                st.session_state.last_output = response
                                st.session_state.messages.append(
                                    {"role": "assistant", "content": response}
                                )

                                append_to_conversation_log(
                                    args.saral_conversation_log,
                                    "assistant",
                                    response,
                                    {"mode": mode},
                                )
                            except Exception as e:
                                st.error(f"Error during generation: {e}")

    # =========================== Evaluation Tab ===========================
    with tab_eval:
        st.subheader("Automated Quality Checks")

        if st.session_state.last_output:
            # ---- Provenance & Factuality Proxy ----
            st.markdown("### Provenance & Factuality Proxy (Last Generation)")
            eval_metrics = TalkEvaluator.evaluate_provenance(
                st.session_state.last_output,
                st.session_state.last_retrieved_docs or [],
            )
            c1, c2, c3, c4 = st.columns(4)
            c1.metric(
                "Citation Coverage",
                f"{eval_metrics['citation_coverage_score'] * 100:.1f}%",
            )
            c2.metric("Lines Analyzed", eval_metrics["total_lines_analyzed"])
            c3.metric("Unique Chunks Cited", len(eval_metrics["unique_chunks_cited"]))
            c4.metric(
                "Avg Lexical Overlap (Cited)",
                f"{eval_metrics['avg_lexical_overlap_cited_lines']:.3f}",
            )
            st.json(eval_metrics)

            st.markdown("---")
            st.markdown("### Script Quality vs Reference (ROUGE & BERTScore)")

            ref_file = st.file_uploader(
                "Upload a reference script (txt/markdown) for comparison",
                type=["txt", "md"],
                key="ref_script_upload",
            )

            if ref_file is not None:
                reference_text = ref_file.read().decode("utf-8", errors="ignore")
                with st.spinner("Computing ROUGE & BERTScore (if available)..."):
                    scores = TalkEvaluator.compute_rouge_and_bertscore(
                        generated=st.session_state.last_output,
                        reference=reference_text,
                    )
                st.json(scores)

                if not HAS_ROUGE or not HAS_BERTSCORE:
                    st.info(
                        "For full metrics, install:\n\n"
                        "- `pip install rouge-score`\n"
                        "- `pip install bert-score`"
                    )

            st.markdown("---")
            st.markdown("### Human Evaluation (3 Raters)")
            with st.form("human_eval_form"):
                st.write(
                    "Rate the last generated talk on a scale of 1 (poor) to 5 (excellent)."
                )

                cols = st.columns(3)
                ratings = {"appropriateness": [], "factuality": [], "helpfulness": []}

                for r in range(1, 4):
                    cols[r - 1].markdown(f"**Rater {r}**")
                    ratings["appropriateness"].append(
                        cols[r - 1].slider(
                            f"Appropriateness (R{r})", 1, 5, 4, key=f"app_{r}"
                        )
                    )
                    ratings["factuality"].append(
                        cols[r - 1].slider(
                            f"Factuality (R{r})", 1, 5, 4, key=f"fact_{r}"
                        )
                    )
                    ratings["helpfulness"].append(
                        cols[r - 1].slider(
                            f"Helpfulness (R{r})", 1, 5, 4, key=f"help_{r}"
                        )
                    )

                submitted = st.form_submit_button("Compute Averages")
                if submitted:
                    avg_app = sum(ratings["appropriateness"]) / 3.0
                    avg_fact = sum(ratings["factuality"]) / 3.0
                    avg_help = sum(ratings["helpfulness"]) / 3.0

                    st.success(
                        f"Average Appropriateness: {avg_app:.2f} | "
                        f"Average Factuality: {avg_fact:.2f} | "
                        f"Average Helpfulness: {avg_help:.2f}"
                    )

                    # Optionally log this to conversation log
                    append_to_conversation_log(
                        args.saral_conversation_log,
                        "human_eval",
                        "manual_ratings",
                        {
                            "avg_appropriateness": avg_app,
                            "avg_factuality": avg_fact,
                            "avg_helpfulness": avg_help,
                            "per_rater": ratings,
                        },
                    )
        else:
            st.info("Generate a response first to see evaluation metrics.")


# ===================================================================
# CLI Entry Point
# ===================================================================
if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="SARAL Chatbot")
    parser.add_argument(
        "--chroma_dir",
        type=str,
        default="./DBs/deepseek_ocr",
        help="Directory for ChromaDB persistence",
    )
    parser.add_argument(
        "--collection_name",
        type=str,
        default="deepseek_ocr_chunks",
        help="Name of the Chroma collection",
    )
    parser.add_argument(
        "--ollama_model",
        type=str,
        default="qwen3:8b",
        help="Ollama model tag to use",
    )
    parser.add_argument(
        "--saral_conversation_log",
        type=str,
        default="conversation.jsonl",
        help="Path to conversation log JSONL file",
    )

    try:
        # parse_known_args because Streamlit injects its own flags
        args, unknown = parser.parse_known_args()
    except SystemExit:
        # Fallback defaults when running via `streamlit run`
        class Defaults:
            chroma_dir = "./chroma_db"
            collection_name = "research_papers"
            ollama_model = "llama3"
            saral_conversation_log = "conversation.jsonl"

        args = Defaults()
        print("SystemExit detected (likely Streamlit --help). Using default arguments.")

    main(args)